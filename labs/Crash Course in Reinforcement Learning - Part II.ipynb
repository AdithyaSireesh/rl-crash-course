{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Crash Course in Reinforcement Learning - Part II.ipynb","provenance":[],"collapsed_sections":["Gw3P1oFTOlnF"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Gw3P1oFTOlnF","colab_type":"text"},"source":["# Setup\n","Run below cells and hide it afterwards with the arrow on the left. "]},{"cell_type":"code","metadata":{"id":"S-4YPGE9O9mH","colab_type":"code","colab":{}},"source":["!pip install gym[Box2D] pyvirtualdisplay pyglet > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_jMNwYlIHTep","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from typing import List, Tuple\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from collections import deque\n","\n","from IPython import display as ipythondisplay\n","from IPython.display import display, update_display, clear_output\n","from time import sleep\n","\n","from pyvirtualdisplay import Display\n","xdisplay = Display(visible=0, size=(1300, 900), backend=\"xvfb\")\n","xdisplay.start()\n","\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","class DoneWrapper(gym.Wrapper):\n","\n","  def step(self, action):\n","    observation, reward, done, info = self.env.step(action) \n","    return observation, reward, False, info\n","      \n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","    \n","def wrap_env(env, done=True):\n","  if not done:\n","    env = DoneWrapper(env)\n","  env = Monitor(env, './video', force=True, mode='evaluation')\n","  return env\n","\n","\n","def print_ansi(screen, display_id='42', wait=0.5):\n","    clear_output(wait=True)\n","    update_display(print(screen.getvalue()), display_id=display_id)\n","    sleep(wait)\n","\n","\n","def plot(img):\n","  fig = plt.figure(figsize=(8,6))\n","  ax = fig.add_subplot(111)\n","  ax.imshow(img)\n","  ax.set_xticks([])\n","  ax.set_yticks([])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ewF-7AdyHTfi","colab":{}},"source":["def gather_trajectories(env: gym.Env, policy, num_trajs: int = 10):\n","    \"\"\"Gather `num_trajs` trajectories by interacting with the environment using the given policy.\"\"\"\n","    \n","    # preapre a list for the trajectories\n","    history = []\n","    \n","    for traj_idx in range(num_trajs):\n","        obs = env.reset()\n","        done = False\n","        current_traj = []\n","        while not done:\n","            \n","            # sample an action from the policy\n","            action = policy.sample(obs)\n","            # feed it into the environment\n","            next_obs, reward, done, _ = env.step(action)\n","            \n","            # save into the history\n","            current_traj += [(obs, action, reward)]\n","\n","            obs = next_obs\n","        history += [current_traj]\n","        \n","    return history\n","\n","def calculate_return(rewards: List[float]) -> Tuple[float, List[float]]:\n","    \"\"\"Calulated and episode and step returns\"\"\"\n","    # calculate the sum of rewards from the episode\n","    rewards = np.array(rewards)\n","    episode_return = np.sum(rewards)\n","    \n","    # prepare a list for the step returns\n","    step_returns = []\n","\n","    # calculate discounted return for each step\n","    # hint: it's easier to go backwards\n","\n","    step_returns = [rewards[-1]]\n","    for reward in reversed(rewards[:-1]):\n","        last_return = step_returns[-1]\n","        step_returns += [reward + last_return]\n","    step_returns.reverse()\n","\n","    return episode_return, step_returns\n","\n","def process_trajectories(history: List):\n","    \"\"\"Process gathered trajectories into tensors and calculate returns\"\"\"\n","    # prepare containers for each element\n","    obs_array = []\n","    action_array = []\n","    return_array = []\n","    episode_returns = []\n","    \n","    # loop over the whole history\n","    rewards = []\n","    for traj_idx, traj in enumerate(history):\n","        # unpack the elements\n","        traj_obs, traj_actions, traj_rewards = list(zip(*traj))\n","\n","        # process the end of an episode - calculate episode and step returns\n","\n","        episode_return, step_returns = calculate_return(traj_rewards)\n","        \n","        episode_returns += [episode_return]\n","        obs_array += traj_obs\n","        action_array += traj_actions\n","        return_array += step_returns\n","\n","    # cast out data to tensors (will be useful later)     \n","    obs_array = torch.tensor(obs_array, dtype=torch.float32)\n","    action_array = torch.tensor(action_array, dtype=torch.float32)\n","    return_array = torch.tensor(return_array, dtype=torch.float32)\n","    episode_returns = torch.tensor(episode_returns, dtype=torch.float32)\n","    \n","    return obs_array, action_array, return_array, episode_returns\n","\n","def visualize(env, policy):\n","    \"\"\"Run the provided policy on the environment\"\"\"\n","\n","    env = wrap_env(env)\n","    obs = env.reset()\n","    done = False\n","    \n","    while not done:\n","        action = policy.sample(obs)\n","        obs, reward, done, _ = env.step(action)\n","        env.render()\n","\n","    env.close()\n","    show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2H_aC5t62gN","colab_type":"text"},"source":["# Part 2. Policy Gradient\n","\n","Here will implement the Policy Gradient algorithm and its necessary components.\n","\n","But before we start we need to add the discount factor trick to out trajectory processing function from the last part.\n","\n","## Exercise: Discounting \n","Add the discount factor to the episode and step returns calculation.\n","We'll set the default value for the gamma parameter so we wouldn't have to change the `process_trajectories` function.\n","\n"]},{"cell_type":"code","metadata":{"id":"InjwPhjS_zBk","colab_type":"code","colab":{}},"source":["def calculate_return(rewards: List[float], gamma: float = 0.99):\n","    \n","    # calculate the *discounted* sum of rewards from the episode\n","    rewards = np.array(rewards)\n","    episode_return = ???\n","    \n","    step_returns = []\n","\n","    # calculate discounted return for each step\n","    # hint: it's easier to go backwards\n","    ???\n","\n","    return episode_return, step_returns\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXd2uitsBgK2","colab_type":"text"},"source":["## Exercise: Network Policy\n","For our Policy Gradient (and further methods) we need a differentiable policy model with optimizable weights - a *Network Policy*.\n","\n","The main specification of the network is already implemented, your task is to add the necessary methods. Same as before, we need our policy to provide us with:\n","* a method to sample action given an observation - `sample` method\n","* a probability vector, like the RandomPolicy - `probs` method.\n","* and additional, a `log_probs` method that returns the same probabilities as the `probs` method, but passed through a logarithm function. It will be useful, since in the formula for policy gradient update we use the logarithm of probability instead of the probability itself."]},{"cell_type":"code","metadata":{"id":"2flZKz1C-lmY","colab_type":"code","colab":{}},"source":["class NetworkPolicy(nn.Module):\n","\n","    def __init__(self, obs_dim: int, action_dim: int, h_dim: int = 16):\n","        super(NetworkPolicy, self).__init__()\n","\n","        self.model = nn.Sequential(nn.Linear(obs_dim, h_dim),\n","                                   nn.Tanh(),\n","                                   nn.Linear(h_dim, action_dim))\n","\n","    def probs(self, obs):\n","        # cast the numpy array to a torch tensor if necessary\n","        obs = ???\n","        # get logits from the model\n","        logits = ???\n","        # use softmax function to transform logits into probability distribution\n","        probs = ???\n","        return probs\n","\n","    def log_probs(self, obs: np.ndarray):\n","        # cast the numpy array to a torch tensor if necessary\n","        obs = ???\n","        # get logits from the model\n","        logits =  ???\n","        # use *log* softmax function to transform logits into probability distribution\n","        log_probs =  ???\n","        return log_probs\n","\n","    def sample(self, obs):\n","        # again, sample from the prepared probability vector \n","        # remember the `.item()` method!\n","        action = ???\n","        return action"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AsnHAh_8_c5D","colab_type":"text"},"source":["## Exercise: Policy Gradient Step and Training\n","\n","Now that we have the necessary elements, we can implement the Policy Gradient itself. Let's start with a single PG step. Your task is to implement the `target` of the Policy Gradient, i.e. the function that we want to optimize.\n","\n","Here's the gradient equation as a remainder:\n","\n","$$ \\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi} R(\\tau) \\approx \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{t=1}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t) R_t = \\nabla_\\theta \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{t=1}^{T}  \\log \\pi_\\theta (a_t | s_t) R_t = \\nabla_\\theta \\hat{J}$$ \n","where action $a_t$, state $s_t$ and step return $R_t$ come from trajectory $\\tau_j$ .\n","\n","PyTorch optimizers by default **minimize** the given function, so your target should be in fact the negative of the loss above (i.e. $-\\hat{J}$).\n","\n","The [`tensor.gather`](https://pytorch.org/docs/stable/torch.html#torch.gather) method may be useful to get the action log-probabilities.\n"]},{"cell_type":"code","metadata":{"id":"nSrc0wOD-G1V","colab_type":"code","colab":{}},"source":["def policy_gradient_step(policy: NetworkPolicy,\n","                         optimizer: torch.optim.Optimizer, \n","                         obs: torch.Tensor, \n","                         actions: torch.Tensor, \n","                         step_returns: torch.Tensor,\n","                         num_trajs: int):\n","\n","    # pass the obs to the policy to get log probabilities of each action\n","    log_probs = ???\n","    \n","    # get the probability of the action thast was actually performed for each observation\n","    action_log_probs = ???\n","    \n","    # calculate the gradient\n","    target = ???\n","    \n","    # pass it to the optimizer\n","    optimizer.zero_grad()\n","    target.backward()\n","    optimizer.step()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWsGCgzNJyyg","colab_type":"text"},"source":["The last missing thing is the training loop that uses all the stuff that we have implemented so far. Write the missing code using the functions and classes that you've implemented already."]},{"cell_type":"code","metadata":{"id":"5_WXntM-BURj","colab_type":"code","colab":{}},"source":["def train_policy_gradient(env: gym.Env, \n","                          policy: torch.nn.Module, \n","                          num_iterations: int = 100, \n","                          trajs_per_gather: int = 10):\n","\n","    # we'll use Adam to update the weights of our network\n","    optimizer = torch.optim.Adam(policy.parameters(), lr=5e-3)\n","    # training loop\n","    for idx in range(num_iterations + 1):\n","        # gather trajectories using current policy\n","        history = ???\n","        \n","        # calculate the obs, actions and returns array by processing the trajectories\n","        obs, actions, step_returns, ep_returns = ???\n","\n","        # policy gradient training\n","        policy_gradient_step(policy=policy,\n","                                optimizer=optimizer,\n","                                obs=obs,\n","                                actions=actions,\n","                                step_returns=step_returns,\n","                                num_trajs=trajs_per_gather)\n","        \n","        # log training progress\n","        if idx % 10 == 0:\n","            print(f\"Traning iteration {idx}, mean episode returns: {ep_returns.mean():.3f}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tk-963wP-ZDD","colab_type":"code","colab":{}},"source":["# moon lander\n","env = gym.make(\"LunarLander-v2\")\n","# or cart pole\n","# env = gym.make(\"CartPole-v1\")\n","\n","# gather necessary dimensions for our netowrk\n","obs_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","# initialize the policy\n","network_policy = NetworkPolicy(obs_dim, action_dim)\n","\n","# train the model\n","train_policy_gradient(env, \n","                      network_policy, \n","                      num_iterations=100,\n","                      trajs_per_gather=20)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7dGihwmKM8Ue","colab_type":"code","colab":{}},"source":["visualize(env, network_policy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzcBzbEDvATu","colab_type":"text"},"source":["# Bonus Exercise 1\n","Try to find an architecture for the agent that allows for the fastest training for:\n","1. CartPole\n","2. LunarLander\n","\n","Are the best architectures the same for both environments?"]},{"cell_type":"markdown","metadata":{"id":"y5dog0bncQhc","colab_type":"text"},"source":["## Bonus Exercise 2\n","\n","It may be tempting to reuse the data we already have. Try to modify `train_policy_gradient` function in order to modify our parameters using the same trajectories multiple times (e.g. call `train_policy_gradient` method 100 times). Does it work? If not, can you think of the reason why?"]}]}