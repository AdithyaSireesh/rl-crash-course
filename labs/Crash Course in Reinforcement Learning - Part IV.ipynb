{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Crash Course in Reinforcement Learning - Part IV","provenance":[],"collapsed_sections":["Gw3P1oFTOlnF"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gw3P1oFTOlnF","colab_type":"text"},"source":["#  Setup\n","Run below cells and hide it afterwards with the arrow on the left. "]},{"cell_type":"code","metadata":{"id":"S-4YPGE9O9mH","colab_type":"code","colab":{}},"source":["!pip install gym[Box2D] pyvirtualdisplay pyglet > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xixLPph_JNnw","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from typing import List, Tuple\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from collections import deque\n","\n","from IPython import display as ipythondisplay\n","from IPython.display import display, update_display, clear_output\n","from time import sleep\n","\n","from pyvirtualdisplay import Display\n","xdisplay = Display(visible=0, size=(1300, 900), backend=\"xvfb\")\n","xdisplay.start()\n","\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","class DoneWrapper(gym.Wrapper):\n","\n","  def step(self, action):\n","    observation, reward, done, info = self.env.step(action) \n","    return observation, reward, False, info\n","      \n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","    \n","def wrap_env(env, done=True):\n","  if not done:\n","    env = DoneWrapper(env)\n","  env = Monitor(env, './video', force=True, mode='evaluation')\n","  return env\n","\n","\n","def print_ansi(screen, display_id='42', wait=0.5):\n","    clear_output(wait=True)\n","    update_display(print(screen.getvalue()), display_id=display_id)\n","    sleep(wait)\n","\n","\n","def plot(img):\n","  fig = plt.figure(figsize=(8,6))\n","  ax = fig.add_subplot(111)\n","  ax.imshow(img)\n","  ax.set_xticks([])\n","  ax.set_yticks([])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w400JFAILnGQ","colab_type":"code","colab":{}},"source":["def gather_trajectories(env: gym.Env, policy, num_trajs: int = 10):\n","    \"\"\"Gather `num_trajs` trajectories by interacting with the environment using the given policy.\"\"\"\n","    \n","    # preapre a list for the trajectories\n","    history = []\n","    \n","    for traj_idx in range(num_trajs):\n","        obs = env.reset()\n","        done = False\n","        current_traj = []\n","        while not done:\n","            \n","            # sample an action from the policy\n","            action = policy.sample(obs) \n","            # feed it into the environment\n","            next_obs, reward, done, _ = env.step(action)\n","            \n","            # save into the history\n","            current_traj += [(obs, action, reward)] \n","\n","            obs = next_obs\n","        history += [current_traj]\n","        \n","    return history\n","\n","\n","def calculate_return(rewards: List[float]) -> Tuple[float, List[float]]:\n","    \"\"\"Calulated and episode and step returns\"\"\"\n","    # calculate the sum of rewards from the episode\n","    rewards = np.array(rewards)\n","    episode_return = np.sum(rewards)\n","    \n","    # prepare a list for the step returns\n","    step_returns = []\n","\n","    # calculate discounted return for each step\n","    # hint: it's easier to go backwards\n","    step_returns = [rewards[-1]]\n","    for reward in reversed(rewards[:-1]):\n","     \n","        last_return = step_returns[-1]\n","        step_returns += [reward + last_return]\n","    step_returns.reverse()\n","\n","    return episode_return, step_returns\n","\n","\n","def process_trajectories(history: List):\n","    \"\"\"Process gathered trajectories into tensors and calculate returns\"\"\"\n","    # prepare containers for each element\n","    obs_array = []\n","    action_array = []\n","    return_array = []\n","    episode_returns = []\n","    \n","    # loop over the whole history\n","    rewards = []\n","    for traj_idx, traj in enumerate(history):\n","        # unpack the elements\n","        traj_obs, traj_actions, traj_rewards = list(zip(*traj))\n","\n","        # process the end of an episode \n","        # - calculate episode and step returns\n","        # ???\n","        episode_return, step_returns = calculate_return(traj_rewards)\n","        \n","        episode_returns += [episode_return]\n","        obs_array += traj_obs\n","        action_array += traj_actions\n","        return_array += step_returns\n","\n","    # cast out data to tensors (will be useful later)     \n","    obs_array = torch.tensor(obs_array, dtype=torch.float32)\n","    action_array = torch.tensor(action_array, dtype=torch.float32)\n","    return_array = torch.tensor(return_array, dtype=torch.float32)\n","    episode_returns = torch.tensor(episode_returns, dtype=torch.float32)\n","    \n","    return obs_array, action_array, return_array, episode_returns\n","\n","\n","def visualize(env, policy):\n","    \"\"\"Run the provided policy on the environment\"\"\"\n","\n","    env = wrap_env(env)\n","    obs = env.reset()\n","    done = False\n","    \n","    while not done:\n","        action = policy.sample(obs)\n","        obs, reward, done, _ = env.step(action)\n","        env.render()\n","\n","    env.close()\n","    show_video()\n","\n","\n","class NetworkPolicy(nn.Module):\n","\n","    def __init__(self, obs_dim: int, action_dim: int, h_dim: int = 16):\n","        super(NetworkPolicy, self).__init__()\n","\n","        self.model = nn.Sequential(nn.Linear(obs_dim, h_dim),\n","                                   nn.Tanh(),\n","                                   nn.Linear(h_dim, action_dim))\n","\n","    def probs(self, obs):\n","        # cast the numpy array to a torch tensor if necessary\n","        if not isinstance(obs, torch.Tensor):\n","            obs = torch.tensor(obs, dtype=torch.float32)\n","        # get logits from the model\n","        logits = self.model(obs) \n","        # use softmax function to transform logits into probability distribution\n","        return F.softmax(logits, -1) \n","\n","    def log_probs(self, obs: np.ndarray):\n","        # cast the numpy array to a torch tensor if necessary\n","        if not isinstance(obs, torch.Tensor):\n","            obs = torch.tensor(obs, dtype=torch.float32)\n","        # get logits from the model\n","        logits = self.model(obs) \n","        # use *log* softmax function to transform logits into probability distribution\n","        return F.log_softmax(logits, -1) \n","        \n","    def sample(self, obs):\n","        # again, sample from the prepared probability vector \n","        # remember the `.item()` method!\n","        probs = self.probs(obs)\n","        return torch.multinomial(probs, 1).item()\n","\n","\n","def policy_gradient_step(policy: NetworkPolicy,\n","                         optimizer: torch.optim.Optimizer, \n","                         obs: torch.Tensor, \n","                         actions: torch.Tensor, \n","                         step_returns: torch.Tensor,\n","                         num_trajs: int):\n","\n","    # pass the obs to the policy to get log probabilities of each action\n","    log_probs = policy.log_probs(obs)\n","    \n","    # get the probability of the action thast was actual performed for each observation\n","    actions = actions.view(-1, 1).long()\n","    action_log_probs = log_probs.gather(1, actions).squeeze()\n","    # calculat the gradient\n","    target = -(action_log_probs * step_returns).sum() / num_trajs\n","    # pass it to the optimizer\n","    optimizer.zero_grad()\n","    target.backward()\n","    optimizer.step()\n","\n","\n","def get_value_network(env: gym.Env, h_dim: int = 32):\n","    \"\"\"Create a value netowrk with 2 hidden layers, both with `h_dim` neurons\n","       and Tanh nonlinear activations\"\"\"\n","\n","    obs_dim = env.observation_space.shape[0]\n","    \n","    # build the network\n","    value_network = nn.Sequential(nn.Linear(obs_dim, h_dim),\n","                                  nn.Tanh(),\n","                                  nn.Linear(h_dim, h_dim),\n","                                  nn.Tanh(),\n","                                  nn.Linear(h_dim, 1))\n","                            \n","    return value_network\n","\n","\n","def value_net_step(obs: torch.Tensor, \n","                   step_returns: torch.Tensor,\n","                   model: torch.nn.Module, \n","                   optim: torch.optim.Optimizer):\n","    \"\"\"\"Train the value network on a single batch of states and returns\"\"\"\n","    \n","    # pass the observatrion to get network and get the predicted values\n","    values = model(obs).squeeze()\n","\n","    # calculate the loss function\n","    loss = ((values - step_returns) ** 2).mean()\n","    \n","    # pass gradeints to the optimizer\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9CfK7hzhkJ8","colab_type":"text"},"source":["# Part 4. Proximal Policy Optimization\n","\n","In this part we'll finally implement the PPO algorithm, similarly to previous steps we'll start with a single algorithm step to then use it in a training function.\n","\n","## Exercise: PPO Step\n","\n","Implement the following PPO step. As a reminder here the formula for PPO update:\n","\n","$$ \\mathcal{L}(\\theta) = \\mathbb{E}_t \\Big[ \\min \\big(\\rho A_t, \\text{clip}(\\rho, 1 - \\epsilon, 1 + \\epsilon) A_t\\big) \\Big] $$\n","\n","$$ \\rho = \\frac{\\pi_\\theta (a_t | s_t)}{\\pi_{\\theta OLD}(a_t | s_t)} $$\n","\n","We approximate the expected value by sampling and in order to maximize this function we need to pass a negative of this function to the PyTorch optimizer, so the loss function that we in fact implement has the form of:\n","$$\n","-\\frac{1}{T} \\sum_i^T \\Big[ \\min \\big(\\rho A_t, \\text{clip}(\\rho, 1 - \\epsilon, 1 + \\epsilon) A_t\\big) \\Big]\n","$$"]},{"cell_type":"code","metadata":{"id":"QgUb5YqDiGnJ","colab_type":"code","colab":{}},"source":["PPO_EPS = 0.2 \n","\n","def ppo_step(policy: torch.nn.Module, \n","             optimizer: torch.optim.Optimizer, \n","             obs: torch.Tensor, \n","             actions: torch.Tensor, \n","             advantages: torch.Tensor, \n","             old_probs: torch.Tensor):\n","    \n","    # get action probs for each possible action from the policy\n","    probs = ???\n","    \n","    # we need to detach the old probs as the updates need to be based on current state only\n","    old_probs.detach()\n","\n","    # gather the probability of the actual action that was taken for each state\n","    action_probs =  ???\n","    \n","    # do the same for old probabilities\n","    old_action_probs = ???\n","    \n","    # callulate rho, the ratio of the current to old probabilities\n","    prob_ratio = ???\n","    \n","    # calculate the unclipped objective, i.e. ratio times advantage\n","    unclipped_objective = ???\n","    # calculate the clipped objective, i.e. clipped ratio times advantage\n","    # hint: torch.clamp\n","    clipped_objective = ???\n","    # calculate the min of the two objectives\n","    # hint: torch.stack\n","    stacked_objectives = ???\n","    objective = ???\n","    # the nial target should be the minus average of the objective\n","    target = ???\n","    \n","    # with PPO target ready we can now pass it to our optimizer\n","    optimizer.zero_grad()\n","    target.backward()\n","    optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fi9vaqaciIKk","colab_type":"text"},"source":["Now that we have our PPO step function we can use it in training, implement the `train_ppo` procedure following the instructions in the comments."]},{"cell_type":"code","metadata":{"id":"7fMizTA8iIXv","colab_type":"code","colab":{}},"source":["def train_ppo(env, policy, value_network, num_iterations=100, batch_size=64, trajs_per_gather=10, num_miniepochs=3):\n","    \n","    # prepare optimizers for both networks\n","    optimizer = torch.optim.Adam(policy.parameters(), lr=5e-3)\n","    value_optimizer = torch.optim.Adam(value_network.parameters(), lr=5e-1)\n","    \n","    # training loop\n","    for idx in range(num_iterations + 1):\n","\n","        # gather trajectories using current policy\n","        history = gather_trajectories(env, policy, trajs_per_gather)\n","        # calculate the obs, actions and returns array by processing the trajectories\n","        obs, actions, rets, ep_returns = process_trajectories(history)\n","        \n","        # get values from the value netowork and calculate the advantage\n","        # don't forget to detach the value network's output from the graph!\n","        values = ???\n","        advs = ???\n","\n","        # randomize the order of trajectory steps for PPO!\n","        indices = torch.randperm(len(obs))\n","        # calculate the required number of batches\n","        batch_num = len(obs) // batch_size\n","\n","        # prepare old probabilities vector \n","        old_probs = policy.probs(obs).detach()\n","        \n","        # ppo traning miniloop\n","        for miniepoch_idx in range(num_miniepochs):\n","            # PPO batches loop\n","            for batch_idx in range(batch_num):\n","\n","                # gather the samples for this batch\n","                batch_start = batch_size * batch_idx\n","                batch_end = batch_start + batch_size\n","                batch_indices = indices[batch_start:batch_end]\n","\n","                # run ppo training step\n","                ppo_step(policy=policy,\n","                         optimizer=optimizer,\n","                         obs=obs[batch_indices],\n","                         actions=actions[batch_indices],\n","                         advantages=advs[batch_indices],\n","                         old_probs=old_probs[batch_indices])\n","                \n","                # run the value network traning step\n","                value_net_step(obs=obs[batch_indices], \n","                               step_returns=rets[batch_indices],\n","                               model=value_network, \n","                               optim=value_optimizer)\n","                \n","        if idx % 10 == 0:\n","            print(f\"Traning iteration {idx}, mean episode returns: {ep_returns.mean():.3f}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9BqFDatcl8S","colab_type":"text"},"source":["All that's left is to run the PPO algorithm on our environment."]},{"cell_type":"code","metadata":{"id":"VNt1ikLg43DI","colab_type":"code","colab":{}},"source":["# moon lander\n","env = gym.make(\"LunarLander-v2\")\n","# or cart pole\n","# env = gym.make(\"CartPole-v1\")\n","\n","# gather necessary dimensions for our netowrk\n","obs_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","\n","# initialize the policy\n","network_policy = NetworkPolicy(obs_dim, action_dim)\n","value_network = get_value_network(env)\n","\n","# train the model\n","train_ppo(env, \n","          policy=network_policy, \n","          value_network=value_network,\n","          num_iterations=50,\n","          trajs_per_gather=20)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZY8NvmrD5C5k","colab_type":"code","colab":{}},"source":["visualize(env, network_policy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlappPjsrSgE","colab_type":"text"},"source":["## Bonus Exercise 1: Compare PPO with vanilla and baseline PG\n","* Compare the required number of samples to reach some desired episode return.\n","* Compare them on the episode return after the same, fixed number of samples seen.  \n","\n","## Bonus Exercise 2: Sampling from a very different agent\n","Sample trajectories using the random policy defined in the first part of this workshop. Try to use them as the \"old policy\" in PPO and train it this way. Does it work? Why or why not? \n","\n","## Uber Bonus Exercise: Different Solutions to Trust Region\n","Can you come up with a different way to force $\\pi_\\theta$ to stay fairly close to its previous values in distribution space than the clipping used in PPO. Experiment with your ideas, see how they perform.\n"]}]}