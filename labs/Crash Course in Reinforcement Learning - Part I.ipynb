{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Crash Course in Reinforcement Learning - Part I","provenance":[],"collapsed_sections":["Gw3P1oFTOlnF"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Gw3P1oFTOlnF","colab_type":"text"},"source":["# Setup\n","Run below cells and hide it afterwards with the arrow on the left. "]},{"cell_type":"code","metadata":{"id":"S-4YPGE9O9mH","colab_type":"code","colab":{}},"source":["!pip install gym[Box2D] pyvirtualdisplay pyglet > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_jMNwYlIHTep","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from typing import List, Tuple\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from collections import deque\n","\n","from IPython import display as ipythondisplay\n","from IPython.display import display, update_display, clear_output\n","from time import sleep\n","\n","from pyvirtualdisplay import Display\n","xdisplay = Display(visible=0, size=(1300, 900), backend=\"xvfb\")\n","xdisplay.start()\n","\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","class DoneWrapper(gym.Wrapper):\n","\n","  def step(self, action):\n","    observation, reward, done, info = self.env.step(action) \n","    return observation, reward, False, info\n","      \n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","    \n","def wrap_env(env, done=True):\n","  if not done:\n","    env = DoneWrapper(env)\n","  env = Monitor(env, './video', force=True, mode='evaluation')\n","  return env\n","\n","\n","def print_ansi(screen, display_id='42', wait=0.5):\n","    clear_output(wait=True)\n","    update_display(print(screen.getvalue()), display_id=display_id)\n","    sleep(wait)\n","\n","\n","def plot(img):\n","  fig = plt.figure(figsize=(8,6))\n","  ax = fig.add_subplot(111)\n","  ax.imshow(img)\n","  ax.set_xticks([])\n","  ax.set_yticks([])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k1U1clC6HTeu"},"source":["# Part 1. The Agent and The Environment\n","Here we are going to familiarize ourselves with basic Reinforcement Learning concepts: environments and agent. \n","\n","Through the whole workshop we will be using the `gym` package for our environment, here's an example of a classic control task."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g0mcOMT_HTew","colab":{}},"source":["import gym\n","\n","# create the environemnt\n","env = gym.make(\"CartPole-v1\")\n","# reset the env and initilize it\n","env.reset()\n","# plot the visualisation\n","plot(env.render(mode='rgb_array'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hPOABUyaHTez","colab":{}},"source":["# state/observation space\n","print(env.observation_space)\n","print(\"max:\", env.observation_space.high)\n","print(\"min:\", env.observation_space.low)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UP9Iv5EARquC","colab_type":"text"},"source":["**Question:** Can you guess what are those four numbers?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VX-JjUrGHTe2","colab":{}},"source":["# action space\n","print(env.action_space)\n","print(\"random action:\", env.action_space.sample())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PsOEU1pFHTe7"},"source":["**Question:** Again, can you guess what are those to actions?\n","\n","Let's see how do we (or specifically the agent) can interact with the environment."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IA96qLRAHTe9","colab":{}},"source":["# set the limit on steps taken\n","max_steps = 200\n","\n","# create the environment again\n","env = wrap_env(gym.make(\"CartPole-v1\"), done=False)\n","# reset it\n","env.reset()\n","# plot\n","env.render()\n","\n","# interact with the environemnt \n","for i in range(max_steps):\n","  \n","    # sample a random action\n","    action = env.action_space.sample()\n","\n","    # act with the sampled action\n","    observation, reward, done, info = env.step(action) \n","\n","    # plot\n","    env.render()\n","  \n","# close the env\n","env.close()\n","# visualise the interaction\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ChCKYSc1HTfC"},"source":["**Mini-exercise:** Change random action to a constant one."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DeDQHLV7HTfD"},"source":["## Exercise: Manual control on a more complicated environment.\n","\n","Try to \"manually\" solve the moon lander problem\n","\n","State space representation:\n","```\n","[x position,   \n"," y position,\n"," x velocity,\n"," y velocity,\n"," lander angle,\n"," angular velocity,\n"," ??,\n"," ??]\n","``` \n","\n","Action space representation:\n","```\n","[no operation\n","left engine,\n","main (bottom) engine,\n","right engine]\n","```"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3E2GXEAsHTfE","colab":{}},"source":["# create the environment (again)\n","env = wrap_env(gym.make(\"LunarLander-v2\"))\n","# reset\n","observation = env.reset()\n","# plot\n","env.render()\n","\n","# interact with the environemnt \n","while True:\n","\n","    # change the constant action here to some heuristic \n","    # base on the state\n","    action: int = 2\n","\n","    # act with the choosen action\n","    observation, reward, done, info = env.step(action) \n","\n","    # plotting\n","    env.render()\n","\n","    # break the loop on game end\n","    if done:\n","        break\n","  \n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tUKt2EUEHTfH"},"source":["## Exercise: Build a class for your Moon Lander policy.\n","\n","We would like to have some abstraction for our Policies, move your solution to the class skeleton below. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Lvl8DVlCHTfI","colab":{}},"source":["class ManualPolicy:\n","\n","  def sample(self, obs: np.ndarray):\n","      \"\"\"Pick an action based on the current state\"\"\"\n","\n","      # move your code here\n","      action = ???\n","      \n","      return action"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7hFzepUhHTfM"},"source":["Use your new policy class in the function below.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZkFC2btxHTfN","colab":{}},"source":["def visualize(env, policy):\n","    \"\"\"Run the provided policy on the environment\"\"\"\n","\n","    env = wrap_env(env)\n","    obs = env.reset()\n","    done = False\n","    \n","    while not done:\n","        action = ???\n","        obs, reward, done, _ = env.step(action)\n","        env.render()\n","\n","    env.close()\n","    show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hTBs0mm8HTfP","colab":{}},"source":["# moon lander\n","env = gym.make(\"LunarLander-v2\")\n","# your policy\n","policy = ManualPolicy()\n","\n","visualize(env, policy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sQWzWkSkHTfS"},"source":["## Exercise: Random Policy\n","We would like to compare our policy to a *random* agent, let's implement another policy class, this time a more generic one that can work for various environments.\n","\n","Implement the `RandomPolicy` class using methods from the PyTorch package.\n","\n","**Hint:** For random sampling you can use [`torch.multinomial`](https://pytorch.org/docs/stable/torch.html#torch.multinomial) function."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"si7rfe_YHTfT","colab":{}},"source":["import torch\n","\n","class RandomPolicy:\n","    \"\"\"Policy that takes uniformly random actions\"\"\"\n","\n","    def __init__(self, action_dim: int):\n","        # save action dimensionality\n","        self.action_dim = action_dim\n","\n","    def probs(self):\n","        # create a tensor with uniform probability for each action\n","        # make sure that the returned type is torch.float\n","        probs = ???\n","\n","        return probs\n","\n","    def sample(self, obs: np.ndarray):\n","        # sample from the prepared probability vector\n","        # * here you can disregard the `obs` parameterm as our random action is \n","        #   independent of the observation\n","        # * because the suggested sampling method returns a tensor, you may need\n","        #   to use the `.item()` method on the returned tensor to get an int  \n","        action = ???\n","\n","        return action\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FZ6V68lHHTfW"},"source":["Test the Random Policy."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ktTRga5aHTfX","colab":{}},"source":["# moon lander\n","env = gym.make(\"LunarLander-v2\")\n","# your policy\n","policy = RandomPolicy(action_dim=env.action_space.n)\n","\n","visualize(env, policy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dsWfOz4_HTfa"},"source":["## Exercise: Gathering Trajectories\n","To compare your Policy to a Random one we need to many *episodes* to get a better picture of how they perform. Here we will gather many trajectories to later compare the performance of the models.\n","\n","\n","Implement the missing code in the function below to gather trajectories, a single element of a trajectory should be a tuple consisting of:\n","`[state, action_taken, reward, done_flag]`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9qTfnx_VHTfc","colab":{}},"source":["def gather_trajectories(env: gym.Env, policy, num_trajs: int = 10):\n","    \"\"\"Gather `num_trajs` trajectories by interacting with the environment using the given policy.\"\"\"\n","    \n","    # prepare a list for the trajectories\n","    history = []\n","    \n","    for traj_idx in range(num_trajs):\n","        obs = env.reset()\n","        done = False\n","        current_traj = []\n","        while not done:\n","            \n","            # sample an action from the policy\n","            action = ???\n","            # feed it into the environment\n","            next_obs, reward, done, _ = ???\n","            \n","            # save obs, action and reward into the history\n","            current_traj += ???\n","\n","            obs = next_obs\n","        history += [current_traj]\n","        \n","    return history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aC2cg6YjHTfe","colab":{}},"source":["env = gym.make(\"LunarLander-v2\")\n","\n","manual_policy = ManualPolicy()\n","random_policy = RandomPolicy(action_dim=env.action_space.n)\n","\n","# this may take a few seconds\n","manual_history = gather_trajectories(env, manual_policy)\n","random_history = gather_trajectories(env, random_policy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0w43UBcoHTfh"},"source":["## Exercise: Processing Trajectories\n","\n","Now that we have the episode records we can process the trajectories to get some statistics, but we need one more thing. We will compare our policies on the sum of rewards and the end of each episode, called *return*. Later we will need a return for each step of the interaction, so let's implement it right now. As a remainder, a step return is defined as:\n","\n","$$ R_t = \\sum_{j=t}^{T} r_j$$\n","\n","and the episode return with the episode trajectory $\\tau$ is simply:\n","\n","$$ R(\\tau) =  \\sum_{j=1}^{T} r_j $$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ewF-7AdyHTfi","colab":{}},"source":["def calculate_return(rewards: List[float]) -> Tuple[float, List[float]]:\n","    \"\"\"Calulate episode and step returns\"\"\"\n","    # calculate the sum of rewards from the episode\n","    rewards = np.array(rewards)\n","    episode_return = np.sum(rewards)\n","    \n","    # prepare a list for the step returns\n","    step_returns = []\n","\n","    # calculate discounted return for each step\n","    # hint: it's easier to go backwards\n","    \n","    ???\n","\n","    return episode_return, step_returns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y4umKZrZHTfl"},"source":["With that function ready we can use it to process our history. Implement the `process_trajectories` function, don't forget to use the `calculate_return` function that you've just implemented."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zwm1U_a2HTfo","colab":{}},"source":["def process_trajectories(history: List):\n","    \"\"\"Process gathered trajectories into tensors and calculate returns\"\"\"\n","    # prepare containers for each element\n","    obs_array = []\n","    action_array = []\n","    return_array = []\n","    episode_returns = []\n","    \n","    # loop over the whole history\n","    rewards = []\n","    for traj_idx, traj in enumerate(history):\n","        # unpack the elements\n","        traj_obs, traj_actions, traj_rewards = list(zip(*traj))\n","\n","        # process the episode - calculate episode and step returns\n","        episode_return, step_returns = ???\n","        \n","        episode_returns += [episode_return]\n","        obs_array += traj_obs\n","        action_array += traj_actions\n","        return_array += step_returns\n","\n","    # cast out data to tensors (will be useful later)     \n","    obs_array = torch.tensor(obs_array, dtype=torch.float32)\n","    action_array = torch.tensor(action_array, dtype=torch.float32)\n","    return_array = torch.tensor(return_array, dtype=torch.float32)\n","    episode_returns = torch.tensor(episode_returns, dtype=torch.float32)\n","    \n","    return obs_array, action_array, return_array, episode_returns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QNB4QJeqHTft"},"source":["With all that we can compare your policy to the random one, will plot here the returns for each consecutive episode."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9qBFVtZZHTfu","colab":{}},"source":["manual_stats = process_trajectories(manual_history)\n","random_stats = process_trajectories(random_history)\n","\n","manual_returns = manual_stats[-1]\n","random_returns = random_stats[-1]\n","\n","plt.boxplot([manual_returns.numpy(), random_returns.numpy()], labels=['manual', 'random'], sym=\"\")\n","\n","print(f\"Average manual policy episode return: {manual_returns.mean():.3f}\")\n","print(f\"Average random policy episode return: {random_returns.mean():.3f}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KwtjJq6nHTfz"},"source":["## Free Play\n","If you want to try to write a better policy here's the whole code in one place."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dUSV3iQ4HTf0","colab":{}},"source":["class ManualPolicy:\n","\n","  def sample(self, obs: np.ndarray):\n","      \"\"\"Pick an action based on the current state\"\"\"\n","\n","      # your code here\n","      action = ???\n","      \n","      return action\n","\n","# moon lander\n","env = gym.make(\"LunarLander-v2\")\n","\n","# random policy\n","policy = RandomPolicy(action_dim=env.action_space.n)\n","# your policy\n","policy = ManualPolicy()\n","\n","# gather trajectories\n","manual_history = gather_trajectories(env, manual_policy)\n","random_history = gather_trajectories(env, random_policy)\n","\n","# process trajectories\n","manual_stats = process_trajectories(manual_history)\n","random_stats = process_trajectories(random_history)\n","\n","# get episode returns\n","manual_returns = manual_stats[-1]\n","random_returns = random_stats[-1]\n","\n","# visualize\n","plt.boxplot([manual_returns.numpy(), random_returns.numpy()], labels=['manual', 'random'], sym=\"\")\n","\n","print(f\"Average manual policy episode return: {manual_returns.mean():.3f}\")\n","print(f\"Average random policy episode return: {random_returns.mean():.3f}\")\n"],"execution_count":0,"outputs":[]}]}